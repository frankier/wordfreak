from snakemake.utils import Paramspace
import pandas
from os.path import join as pjoin
from os import chdir


DOWNLOADS = "downloads"
OUTPUTS = "outputs"


COOKIES_MESSAGE = """
First go to https://korp.csc.fi/korp/ -- sign in and agree to the terms of the
the needed corpora and then save your korp cookie as a txt file to {}. You can
use e.g. https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/
"""


WMT_NEWSCRAWL_URLS = [f"http://newscrawl:acrawl4me@data.statmt.org/news-crawl/doc/wmt19/en-doc/news-docs.{year}.en.filtered.gz" for year in range(2007, 2019)]


df = pandas.DataFrame.from_records(
    columns=["lang", "genre", "corpus", "urls", "type", "has_lemmas"],
    data=[
        ("fi", "encylopedia", "wikipedia", ["https://korp.csc.fi/download/wikipedia-fi/wikipedia-fi-2017-src/wikipedia-fi-2017-src.zip"], "vrt", True),
        ("fi", "forum", "suomi24", ["https://korp.csc.fi/download/Suomi24/2001-2017/suomi24-2001-2017-vrt-v1-2.zip"], "vrt", True),
        ("fi", "legal", "acquis-ftb3", ["https://korp.csc.fi/download/acquis-ftb3/acquis-ftb3.zip"], "vrt", True), # Is it VRT?
        ("fi", "legal", "finlex", ["https://opus.nlpl.eu/download.php?f=Finlex/v2018/parsed/fi.zip"], "opus-finlex", True),
        ("fi", "legal", "europarl", ["https://opus.nlpl.eu/download.php?f=Europarl/v8/parsed/fi.zip"], "opus-europarl", True),
        ("fi", "news", "yle", ["https://korp.csc.fi/download/YLE/fi/.zip/ylenews-fi-2011-2018-vrt.zip"], "vrt", True),
        ("fi", "news", "stt", ["https://korp.csc.fi/download/STT/stt-fi-1992-2018-conllu-src/stt-fi-1992-2018-conllu-src.zip"], "conllu", True),
        ("fi", "subtitles", "opensub18", ["https://korp.csc.fi/download/opensubtitles-fi/opensub-fi-2017-src/opensub-fi-2017-src.zip"], "vrt", True),
        ("en", "legal", "europarl", ["https://opus.nlpl.eu/download.php?f=Europarl/v8/parsed/en.zip"], "opus-europarl", True),
        ("en", "news", "newscrawl", WMT_NEWSCRAWL_URLS, "wmt19", False),
        ("en", "subtitles", "opensub18", ["https://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/parsed/en.zip"], "opus-opensub18", True),
        # Could add e.g. Tatoeba or LING-8 but would need to somehow get fake documents (use users?)
        # https://opus.nlpl.eu/download.php?f=Tatoeba/v2021-07-22/parsed/en.zip
    ]
)
flat_df = df.explode(["urls"]).rename(columns={"urls": "url"})
flat_df["basename"] = flat_df["url"].map(lambda url: url.rsplit("/", 1)[-1])
full_paramspace = Paramspace(df)
full_flat_paramspace = Paramspace(flat_df)
df_key = ["is_korp", "lang", "genre", "corpus", "basename"]
corpus_key = df_key[1:-1]
#basename_url_lookup = dict(zip(flat_df[df_key], flat_df["url"]))
is_korp = flat_df["url"].str.startswith("https://korp.csc.fi/")
flat_df.insert(0, "is_korp", is_korp)
basename_df = flat_df[df_key]
basename_paramspace = Paramspace(basename_df)
korp_basename_df = basename_df[is_korp][df_key[1:]]
other_basename_df = basename_df[~is_korp][df_key[1:]]
korp_paramspace = Paramspace(korp_basename_df)
other_paramspace = Paramspace(other_basename_df)
corpus_paramspace = Paramspace(df[corpus_key])


def pick_df_dict(df, dic):
    return df[(df[list(dic.keys())] == pandas.Series(dic)).all(axis=1)]


rule all_divergences:
    input:
        expand(pjoin(OUTPUTS, "{params}.parquet"), params=corpus_paramspace.instance_patterns)


rule cookies_explanation:
    output:
        "korp_cookies.txt"
    run:
        print(COOKIES_MESSAGE.format(output))
        sys.exit(-1)


def get_url(wc):
    inst = full_flat_paramspace.instance(wc)
    sub_df = pick_df_dict(flat_df, inst)
    res = sub_df["url"].iloc[0]
    return res


rule download_korp:
    input:
        "korp_cookies.txt"
    output:
        pjoin(DOWNLOADS, "is_korp~True", korp_paramspace.wildcard_pattern)
    params:
        url = get_url
    shell:
        "mkdir -p $(dirname {output}) && " + \
        " wget --load-cookies {input} " + \
        " {params.url}" + \
        " -O {output}"


rule download_other:
    output:
        pjoin(DOWNLOADS, "is_korp~False", other_paramspace.wildcard_pattern)
    params:
        url = get_url
    shell:
        "mkdir -p $(dirname {output}) && " + \
        " wget " + \
        " {params.url}" + \
        " -O {output}"


def divergences_input(wildcards):
    corpus_row = corpus_paramspace.instance(wildcards)
    sub_df = pick_df_dict(flat_df, corpus_row)[df_key]
    return [pjoin(DOWNLOADS, inst) for inst in Paramspace(sub_df).instance_patterns]


rule download_all:
    input:
        [pjoin(DOWNLOADS, inst) for inst in basename_paramspace.instance_patterns]


# TODO: join multi-file corpora at this point
rule get_divergences:
    input:
        divergences_input
    params:
        corpus_row = lambda wc: pick_df_dict(basename_df, corpus_paramspace.instance(wc))
    output:
        pjoin(OUTPUTS, corpus_paramspace.wildcard_pattern + ".parquet")
    run:
        shell(
            "mk_disp " + ("--lemma" if params.corpus_row["has_lemmas"] else "") +
            " --corpus-type " + params.corpus_row["type"] +
            " {output}" +
            " {input}"
        )
